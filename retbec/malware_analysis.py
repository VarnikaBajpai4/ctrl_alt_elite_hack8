import json
import re
import os
import sys
from dotenv import load_dotenv
import google.generativeai as genai

# Load environment variables
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

def analyze_malware(mal_dir):
    # Find the C file in the directory
    c_files = [f for f in os.listdir(mal_dir) if f.endswith('.c')]
    if not c_files:
        return {"error": "No C file found in the directory"}
    
    c_file = c_files[0]
    features_path = os.path.join(mal_dir, "features.json")
    c_code_path = os.path.join(mal_dir, c_file)

    if not os.path.exists(features_path):
        return {"error": f"Features file not found at {features_path}"}
    if not os.path.exists(c_code_path):
        return {"error": f"C file not found at {c_code_path}"}

    # Load the features JSON
    with open(features_path, "r") as f:
        features = json.load(f)

    # Load and check C code size
    with open(c_code_path, "r", errors="ignore") as f:
        c_code = f.read()
        lines = c_code.splitlines()
        if len(lines) > 50000:
            c_code = "\n".join(lines[:50000]) + "\n// ... [Code truncated due to size limit] ..."
    
    # Construct the prompt
    prompt = f"""
    You are a highly advanced malware analyst reviewing a Windows binary using static analysis (C code and extracted features). Your task is to determine if the binary is **malicious or benign**, factoring in **intent, context, and stealth behavior**.

    ---

    ### 1. API Usage and Intent
    - Do **not** treat APIs like `CreateFileW`, `CreateThread`, `RegSetValueExW`, or `OpenProcess` as malicious **unless** they appear in **specific suspicious sequences**.
    - Suspicious API **chains** include:
    - `VirtualAlloc` → `WriteProcessMemory` → `CreateRemoteThread`
    - `LoadLibrary` → `GetProcAddress` (without direct imports) → `registry manipulation`
    - Registry edits + file writes into `System32`, `%AppData%`, or `%TEMP%` without user interaction
    - **Context is key**: if these APIs are used during legitimate service installation (e.g., `CreateServiceW`, `SHFileOperationW`, etc.), treat as **benign** unless other red flags are present.

    ---

    ### 2. Trust Indicators (Downgrade Risk)
    Reduce malware probability if:
    - `has_signature` is true and the binary modifies system keys under `HKLM\\System\\CurrentControlSet\\Services`
    - `has_debug` is true and `exports == 0`
    - `subsystem` is `WINDOWS_GUI` or `WINDOWS_CUI`, and typical GUI APIs (DialogBox, MessageBox, etc.) are used
    - The file shows **no code injection** (e.g., `VirtualAlloc`, `WriteProcessMemory`) or **anti-analysis** (`IsDebuggerPresent`, entropy > 7.0 in `.rsrc`/`.text`)

    ---

    ### 3. High-Risk Malware Behavior (Increase Risk)
    Raise suspicion significantly if:
    - `VirtualAlloc` + `CreateRemoteThread` appear
    - Obfuscation techniques are detected (high entropy sections: `.text` > 6.7, `.rsrc` > 6.0)
    - File operations target **auto-start** locations or attempt **persistence**
    - Uses network calls to unknown domains or raw socket APIs
    - More than 15 registry or file manipulation calls without UI interactions

    ---

    ### 4. Stealthy Installer / Dropper Heuristics
    If the binary:
    - Has `LoadLibrary` + `GetProcAddress` but no direct imports
    - Performs only registry and file activity, avoids UI APIs
    - Has 2000+ strings with average length < 7 (potential unpacked payload)
    - `.text` entropy > 6.3 and no digital signature

    Then:
    - Increase malware probability to **at least 0.6**, even if no code injection is found

    ---

    ### 5. Final Scoring Logic
    - Start at 0.0
    - Add 0.3 for each matching suspicious chain or stealth indicator
    - Subtract 0.2 if signed and fits known service/installer behavior
    - Cap malware_probability between 0.0 and 1.0
    - Set confidence_score higher if more features or indicators are available

    ---

    Return ONLY this JSON format (no extra commentary):

    {{
    "malware_probability": <float between 0.0 and 1.0>,
    "suspicious_indicators": [<list of specific findings>],
    "behavioral_patterns": [<list of observed behavior chains or categories>],
    "potential_impact": <string describing threat OR benign purpose>,
    "confidence_score": <float between 0.0 and 1.0>
    }}

    C code:
    {c_code}

    Features:
    {json.dumps(features, indent=2)}
    """






    try:
        model = genai.GenerativeModel("models/gemini-1.5-flash")
        response = model.generate_content(prompt)

        match = re.search(r"\{[\s\S]*\}", response.text)
        if match:
            return json.loads(match.group(0))
        return {"error": "Invalid response format from Gemini"}

    except Exception as e:
        return {"error": f"Analysis failed: {str(e)}"}

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python malware_analysis.py <path-to-retdec-output-dir>")
        sys.exit(1)
    
    result = analyze_malware(sys.argv[1])
    print(json.dumps(result, indent=2))